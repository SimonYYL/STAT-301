{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f9bc2a2-a32b-47e2-9e5d-9ca04e4ea6f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.4     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.4.4     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.3     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n",
      "\n",
      "Attaching package: ‘cowplot’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:lubridate’:\n",
      "\n",
      "    stamp\n",
      "\n",
      "\n",
      "Registered S3 method overwritten by 'GGally':\n",
      "  method from   \n",
      "  +.gg   ggplot2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(infer)\n",
    "library(cowplot)\n",
    "library(broom)\n",
    "library(dplyr)\n",
    "library(knitr)\n",
    "library(GGally)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe2789-5e07-4768-a32c-d252f0428a45",
   "metadata": {},
   "source": [
    "# Methods and Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126d2701-a7f1-406b-9b70-cc598b990fb4",
   "metadata": {},
   "source": [
    "## Method:\n",
    "The method chosen to address my question is **Gradient Boosting Machines** (GBM). <br>\n",
    "GBM is a machine learning techique, based on **decision trees**, which combines mutipile weak learners to create a strong predictive model. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f633f62-3995-42bf-85d9-f4596b4be677",
   "metadata": {},
   "source": [
    "## Why is this method appropriate?\n",
    "**In general**, GBM is appropriate to address our question because it can handle both numerical and categorical variables, and it can model complex non-linear relationships between the predictors and the outcome vaiable. <br>\n",
    "**More specifically** , GBM is ideal choice for this project for several reasons:\n",
    "1. **Handling Mixed Data Types**: By exploying the data we know that our dataset contains both categprical (e.g., job title, location) and numerical (e.g.,working year) variables. GBM can handle both types of data without the need for transformation.\n",
    "2. **Non-linear Relationships**: By exploying the dataset we know that the relationship between the predictors and the response variable are not linear. GBM can model complex non-linear relationship.\n",
    "3. **Handle Outliers**: By exploying the dataset we know that many of our perdictors contain outliers. This would be a potential issue which may affect our accuracy of perdication. GBM is robust to outliers, which means it can provide reliable predictions even when the data containing extreme values.\n",
    "4. **Performance**: GBM is known for its predictive accuracy and generally performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee11e0-4925-48d4-abc7-fbfbd3d79f08",
   "metadata": {},
   "source": [
    "## Which assumptions are required, if any, to apply the method selected?\n",
    "- No Specific Distributional Assumptions: Unlike some other algorithms, GBM does not make specific distributional assumptions about the data.\n",
    "- Independence of Errors: GBM assumes that the errors are independently and identically distributed.\n",
    "- Learning rate is acrucial parameter, which means we assume it will help optimizing the model's performance by adjust the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ac3223-f241-4250-89d9-ff48769a6281",
   "metadata": {},
   "source": [
    "## What are potential limitations or weaknesses of the method selected?\n",
    "- GBM can be computational intenseive, since UBC's server may not be able to handle complex models, this should grab attentions.\n",
    "- Overfitting Risk: GBM can overfit the training data if not properly regularized or if the number of iterations is too high1. This means the model might perform well on the training data but poorly on new, unseen data.\n",
    "- Sensitive to Noisy Data: Although GBM is robust to outliers in the output (y), it can be sensitive to noise in the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875a962-4c1b-42f7-9c24-4895fb646465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
